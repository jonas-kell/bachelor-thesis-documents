Every building block inside a sophisticated neural network contributes to the architecture's inductive bias as a whole, starting from the most basic ones.

\paragraph{Fully connected Layers} have a relatively weak bias, because all inputs relate to all outputs independently \cite{relationalInductiveBiasesAndGraphNetworks}. 
The motivation is to construct a block with relatively high flexibility.
It is barely restricted on the type of solution and can therefore easily be used alongside other computational elements.
In conjunction with a non-linear element (like ReLU) the architecture resembles the structure of the human brain. 
This motivates the assumption, that the element can perform useful computations.
In the field of nqs, the \emph{restricted Boltzmann machine} is often employed.
The RBM basically consists of fully connected layers and can be motivated to be very well suited to model the statistical interactions of many body quantum mechanics \cite{restrictedBoltzmanMachines}. 

\paragraph{Convolutions} work with smaller kernels that get shifted over a larger input space.
This implies the two biases of \emph{locality} and \emph{translation} \cite{relationalInductiveBiasesAndGraphNetworks}.
That means, things close together relate more than things far apart. 
Something that in most cases is very true for images (if a dogs ears are detected, but far apart from each other, this most likely isn't a dog. If they are detected next to each other, this is a good indication for a dog).
Also it doesn't matter, where things are located on the image (a dog in the left corner id a dog in the same way as one on the right).

\paragraph{Symmetric convolutions} have the same biases as their generic counterparts. They however have the additional bias of \emph{only regarding the distance, not the direction}.
This helpful, if that data shows dependencies on the proximity of elements in relation to each other. 
The Ising lattices, reviewed in this thesis, have this property. 
If two spins interact or not is only dictated by how close they are.
Not by the direction one is from the other.

\paragraph{Recurrent layers} used a lot in translation tasks and video analysis, because of their \emph{time invariance} \cite{relationalInductiveBiasesAndGraphNetworks}. 
These would likely be very powerful for investigating the development of a quantum system over time. 
AS this thesis focuses on stationary problems, recurrency is not used.

\paragraph{Residual connections} are used for a value to be able state to bypass a block completely.
They are motivated by the bias that it is simpler to zero out an element in a machine learning architecture, than to learn the \emph{identity operation}. 
So if the network recognizes a block does more harm than good in terms of getting the correct solution, it can easily be bypassed by zeroing the block's inputs and using the residual connection. 
At the same time it is unlikely for the element to hurt, as basically each building block can be trained to learn the identity operation, but if a block is not needed it only generates training errors, no additional value \cite{deepResidualLearningForImageRecognition}.
As this element is very likely to be useful in deep networks like transformers, all the transformer architectures in the performed experiments use residual connections.

\paragraph{Pooling operators} have many similarities to convolutions. The however have the added bias, that the relation does not even depend on the position of a value inside the kernels influence. 
It solely depends on the relative values. 
If this true for the data and the answer really only depends on for example the \emph{average} of the values, this module greatly increases computational speed and decreases the required memory.
This on the other hand allows for evaluating larger/deeper networks which may be desirable.