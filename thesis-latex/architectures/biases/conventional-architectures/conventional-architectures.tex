Every building block inside a sophisticated neural network contributes to the architecture's inductive bias as a whole, starting from the most basic ones.

\paragraph{Fully connected Layers} have a relatively weak bias, because all inputs relate to all outputs independently \cite{relationalInductiveBiasesAndGraphNetworks}. 
The motivation is to construct a block with relatively high flexibility.
It is barely restricted on the sort of data it can process and can therefore easily be used alongside other computational elements.
In conjunction with a non-linear element (like ReLU), the architecture resembles the structure of the human brain. 
This motivates the assumption, that the element can perform useful computations.
In the field of NQS, the \emph{restricted Boltzmann machine} is often employed.
The RBM basically consists of fully connected layers and can be motivated to be very well suited to model the statistical interactions of many body quantum mechanics \cite{restrictedBoltzmanMachines}. 

\paragraph{Convolutions} work with smaller kernels that get shifted over a larger input space.
This implies the two biases of \emph{locality} and \emph{translation} \cite{relationalInductiveBiasesAndGraphNetworks}.
That means, things close together relate more than things far apart. 
Something that in most cases is very true for images (if a dog's ears are detected, but far apart from each other, this most likely isn't a dog. If they are detected next to each other, this is a good indication for a dog).
Also it doesn't matter, where things are located on the image (a dog in the left corner is a dog in the same way as one on the right).

\paragraph{Symmetric convolutions} have the same biases as their generic counterparts. They however have the additional bias of \emph{only regarding the distance, not the direction}, which implies a kind of \emph{rotational} symmetry.
This helpful, if that data shows dependencies on the proximity of elements in relation to each other. 
The Ising lattices, reviewed in this thesis, have this property. 
If two spins interact or not is only dictated by how close they are, not by the direction one is from the other.

\paragraph{Depthwise (separable) convolutions} were introduced by research on how to efficiently reduce network size \cite{mobileNetPaper}.
Their bias is that interaction between values of one channel (for example one color channel in the case of rgb-images) can be computed independent and then combined in a second step.
This makes sense, as there is already quite a lot of information inside one color channel. 
And as extracting the information per channel before comparing across channels saves a lot of computational resources, this is a logical step to take.
Also it should in most cases be possible to recognize e.g. a dogs shape only from one color channel. 
Therefore checking for a shape in all channels separately and then comparing seems easier than forcefully training to assemble shapes across channels.

\paragraph{Recurrent layers} are used a lot in translation tasks and video analysis, because of their \emph{time invariance} \cite{relationalInductiveBiasesAndGraphNetworks}. 
These would likely be very powerful for investigating the development of a quantum system over time. 
As this thesis focuses on stationary problems, recurrency is not used.

\paragraph{Residual connections} are used for a value in a calculation to be able to bypass a block completely.
The metaformer architecture in \autoref{fig:metaformer-architecture} includes residual connections as an important part.
They are motivated by the bias, that it is simpler to zero out an element in a machine learning architecture, than to learn the \emph{identity operation}. 
So if the network recognizes a block does more harm than good in terms of getting the correct solution, it can easily be bypassed by zeroing the block's inputs and using the residual connection. 
At the same time it is unlikely for the element to hurt, as basically each building block can be trained to learn the identity operation, but if a block is not needed it only generates training errors, no additional value \cite{deepResidualLearningForImageRecognition}.
As this element is very likely to be useful in deep networks like metaformers, all the metaformer based architectures in the performed experiments use residual connections.

\paragraph{Pooling operators} have many similarities to convolutions. They however have the added bias, that the relation does not even depend on the position of a value inside the kernels influence. 
It solely depends on the relative values. 
If this true for the data and the answer really only depends on for example the \emph{average} of the values, this module greatly increases computational speed and decreases the required memory.
This on the other hand allows for evaluating larger/deeper networks which may be desirable.