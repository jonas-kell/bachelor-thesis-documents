An \emph{inductive bias} is a necessary assumption or rule that allows an algorithm to make an inductive leap and generalize knowledge to be able to evaluate previously unseen data \cite{needForBias}.
The bias can result from the used algorithm or from the underlying architecture.
Having a bias is not optional, because without a bias an algorithm can never \glqq learn\grqq{}, only memorize previously seen examples.
Generally biases can be seen as constraints, that make a procedure \emph{prioritize one solution over an other} \cite[]{relationalInductiveBiasesAndGraphNetworks}. 

It is obvious, that some biases are better suited for learning the solution to a task than others. 
The bias \glqq every picture is a cat\grqq{}, allows an algorithm to judge any picture. 
However it will probably decide wrong most of the time.

Different machine learning architectures are motivated by different inductive biases. 
Because of that, they may perform differently on the same task.
While (as previously established) it is in theory possible to learn every function with a large enough neural network, in practice some network designs outperform others by quite a margin.