% TODO
% - what is dot product attention
% - mathematical bases 
% - motivated by language processing (attention is all you need and BERT)
% - was employed for image processing (image is worth, swin transformer)
% - positional encoding (learned and sinusoidal)
% - output generation (class token vs avg convolution) 
% - need for positional encoding 


% SOURCE: \cite{attentionIsAllYouNeed}
% - General paper to cite for attention mechanism 

% SOURCE: \cite{imageWorth16x16}
% - token embedding for images

% SOURCE: \cite{swinTransformerPaper}
% - improvements in image processing 

% SOURCE: \cite{positionalEncodingGithub}
% - cite here implementation package