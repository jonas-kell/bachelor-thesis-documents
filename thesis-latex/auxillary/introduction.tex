AI-Architectures are rapidly evolving. With the surging availability of data due to cheap storage \cite*[]{costDataStorage} and high resolution sensors in every device, the need for processing said information rises alongside the trend.
As every single person is producing data at an unprecedented rate \cite*[]{dataCreatedRate}, the growing interconnectedness due to advances in social media and the internet fuels the need to not only process the data one self is producing, but also the data others are generating.
Corporations are depending on high throughput and high accuracy data extraction, to allow them to provide their services \cite*[]{marketsizeArtificialIntelligence}.
At the same time, individuals call for faster and more accurate processing of their voice and images by computational intelligences. 

Due to the likewise surging developments in GPU and TPU computing \cite*[]{gpuPerformanceOverTime}, bigger and more elaborate neural networks can be evaluated.
This however most of the time comes with huge requirements in hardware and power. 
Requirements that often times can only be efficiently met in datacenter-scale computations.

This however harshly conflicts with the populations demand for privacy. 
Processing of highly confidential data should best only be performed offline and locally on the users device.

In order to meet these demands for high performance and low footprint networks, very specialized network-architectures have been developed over the past years \cite[]{attentionIsAllYouNeed, metaformerPaper}. 
These employ highly technical inductive biases to allow for calculations, that are precisely tailored to the individual task - therefore becoming more performant while remaining or even becoming less computationally expensive \cite*[]{mobileNetPaper}.

In the domain of many body physics exist numerous challenges to come up with better and more efficient ways to compute the behavior of quantum systems.
With the aid of computers, exact solutions could \glqq simply\grqq{} be calculated numerically.
However as the exponential nature of the tasks at hand is still vastly overshadowing the advances in computational availability \cite*[]{quantumMonteCarloSimulationsOfSolids}, the focus was shifted to more involved ways of solving this problem.

The \emph{neural quantum state} as a method to combat the problem with the use neural networks is by now firmly established as a more than capable approach \cite*[]{quantumMBPwithneuralNetworks}. However the models used in these computations tend to be of relatively modest and seemingly \glqq outdated\grqq{} nature in comparison to the bleeding edge of computational AI.

This thesis is supposed to take a look at the feasibility and usefulness of employing highly specialized, modern neural network architectures, with specifically tailored inductive biases, to hopefully enrich the current computational methods. 
The target is to compute the \emph{ground state} of a system described by the \emph{transverse field Ising model} \cite*[]{isingBook} in 1D as well as 2D lattices.
In the course of this, the focus should lie on the \emph{metaformer} architecture \cite*[]{metaformerPaper}, \emph{graph networks} \cite*[]{relationalInductiveBiasesAndGraphNetworks} and the unification of the two concepts.
