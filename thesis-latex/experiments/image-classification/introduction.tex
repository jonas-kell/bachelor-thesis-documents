The first set of experiments will be on the image classification task.
The goal is to compare the architectural advantages and drawbacks between multiple different kinds of metaformers.

Training was be performed on a subset of 100 classes from the \emph{ImageNet} dataset \cite{imagenetDataset} ($\approx \SI{126000}{}$ images).
The accompanying code to replicate these experiments can be found on GitHub \cite{selfComputerScience}.

The neural networks, as well as the training and evaluation code were written in python.
The \emph{PyTorch} \cite{pytorchGithub} machine learning framework was used as a measure to efficiently define neural networks and lever the computational capabilities of parallelization via GPUs.

The main metaformer model was originally based on the vision transformer found in an implementation of \emph{DINO} \cite{dinoGithub} and modified strongly.  
The code base provides a DINO implementation with as little modifications as possible to compare to own models.
Also the code for comparing against a pre-implemented \emph{poolformer} \cite{poolformerGithub} is provided.

The \emph{einops} package \cite{einopsGithub} is used in multiple locations.
It provides tensor operations, configurable with the \emph{Einstein notation} and simplifies the notation and subsequently readability.
Lastly, the implementation of the \emph{positional encoding} is provided by \cite{positionalEncodingGithub}.
