Just as described in \autoref{sec:architectures-attention}, the transformer architecture's performance can be improved by providing it \emph{positional encoding} (pe) information.
While with convolutions and pooling, identical patches at different locations can be distinguished, in the attention mechanism they can't.
Adding pe should (end does) improve this.

The question is, whether the \emph{masked} attention, that uses the graph information to confine the transformer to only its local neighborhood, also requires pe or not. 
And if efficient training requires pe, what type works best.

\begin{figure}[htbp]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{./experiments/image-classification/positional-encoding/pe-comparison/pe-comparison.pdf}}
    \caption{Visualization of the training attention and loss metrics for three different models. \emph{TF}, \emph{GTF-NN} and \emph{GTF-NNN}.
    Only part of the full training is shown. 
    Each model was trained once with \emph{sinusoidal} pe, \emph{learned} pe and \emph{without} any additional pe.
    Apart from that all hyperparameters are kept static.
    }
    \label{fig:positional-encoding-training}
\end{figure}

The experiments are visualized in \autoref{fig:positional-encoding-training}.
It can be clearly seen, that the different architectures react differently to the types of pe.

The data clearly shows benefits in employing pe. 
All models show an increase in accuracy of about \SIrange[]{3}{4}{\percent}. 
This is less than the difference between some of the other token mixing elements, but still a significant margin in comparison to the absolute performance.
As adding pe is rather cheap, the employment of this strategy can definitely be useful:
Both types of pe are only added at the start of the calculation.
Therefore the cost is not dependent on the depth or type of token-mixer.
Calculating sinusoidal pe can theoretically be done once and the result can be cached, rendering it basically instantaneous to employ and of no negative computational impact. Furthermore it is resolution independent, as the sinus curves can later be sampled with a smaller interval in order to increase network resolution retrospectively.
Learned sinusoidal encodings on the other hand require a backwards propagation of error values into the encoding weights.
This requires additional computation and storage space, as well as providing no obvious way to update the encoding resolution after the fact.

The pure transformer definitely performs best with a learned pe. 
Not only is the overall accuracy higher ($\approx \SI[]{1.5}[]{\percent}$), but loss-minimum is reached about 12 epochs earlier - a huge potential reduction in necessary computation.

The GTF-NN model shows less benefit from applying pe.
This makes sense, as by design the number of attention targets is intentionally very limited. 
Sinusoidal pe and learned pe perform very similar, with sinusoidal pe converging ever so slightly faster than learned.
It should be noted though, that the sample size is too limited to draw definite conclusions from this tiny difference.

In the GTF-NNN, the learned pe outperforms the sinusoidal one, taking about 5 epochs longer to converge, but accomplishing an about \SI[]{2}{\percent} better accuracy.

To summarize, the pe can definitely improve the performance of an attention based neural network. 
That this was the case for normal attention was already know \cite{attentionIsAllYouNeed, imageWorth16x16}. 
Now this small scale experiment shows, that the correlation can hold true for the attention mechanism with graph-limited influence.

A challenge in more complex graph structures is undoubtably the definition of suitable pes, as the canonical sinusoidal encoding is not  extendable to irregular graphs.
The learned pe would probably constitute the easiest and best solution, but comes with a computational overhead both in terms of the cost of one epoch, as well as the speed of convergence (needed number of epochs until converged).

In the ground state search task, no positional encoding is applied. 
This could be a target for future research, especially on highly irregular graphs.