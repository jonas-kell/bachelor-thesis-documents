@article{convNetForThe2020s,
  doi = {10.48550/ARXIV.2201.03545},
  url = {https://arxiv.org/abs/2201.03545},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A ConvNet for the 2020s},
  publisher = {arXiv},
  year = {2022},  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{imageWorth16x16,
  doi = {10.48550/ARXIV.2010.11929},
  url = {https://arxiv.org/abs/2010.11929},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{attentionIsAllYouNeed,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bertPaper,
  doi = {10.48550/ARXIV.1810.04805},
  url = {https://arxiv.org/abs/1810.04805},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{channelNets,
  doi = {10.48550/ARXIV.1809.01330},
  url = {https://arxiv.org/abs/1809.01330},
  author = {Gao, Hongyang and Wang, Zhengyang and Ji, Shuiwang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{deepComplexNetworks,
  doi = {10.48550/ARXIV.1705.09792},
  url = {https://arxiv.org/abs/1705.09792},
  author = {Trabelsi, Chiheb and Bilaniuk, Olexa and Zhang, Ying and Serdyuk, Dmitriy and Subramanian, Sandeep and Santos, João Felipe and Mehri, Soroush and Rostamzadeh, Negar and Bengio, Yoshua and Pal, Christopher J},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Complex Networks},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{deepResidualLearningForImageRecognition,
  doi = {10.48550/ARXIV.1512.03385},
  url = {https://arxiv.org/abs/1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep Residual Learning for Image Recognition},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{dinoPaper,
  doi = {10.48550/ARXIV.2104.14294},
  url = {https://arxiv.org/abs/2104.14294},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Emerging Properties in Self-Supervised Vision Transformers},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{jVMCPaper,
  doi = {10.48550/ARXIV.2108.03409},
  url = {https://arxiv.org/abs/2108.03409},
  author = {Schmitt, Markus and Reh, Moritz},
  keywords = {Computational Physics (physics.comp-ph), Disordered Systems and Neural Networks (cond-mat.dis-nn), Strongly Correlated Electrons (cond-mat.str-el), FOS: Physical sciences, FOS: Physical sciences},
  title = {jVMC: Versatile and performant variational Monte Carlo leveraging automated differentiation and GPU acceleration},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{metaformerPaper,
  doi = {10.48550/ARXIV.2111.11418},
  url = {https://arxiv.org/abs/2111.11418},
  author = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MetaFormer Is Actually What You Need for Vision},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mobileNetPaper,
  doi = {10.48550/ARXIV.1704.04861},
  url = {https://arxiv.org/abs/1704.04861},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{relationalInductiveBiasesAndGraphNetworks,
  doi = {10.48550/ARXIV.1806.01261},
  url = {https://arxiv.org/abs/1806.01261},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Relational inductive biases, deep learning, and graph networks},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{restrictedBoltzmanMachines,
  doi = {10.1038/s41567-019-0545-1},
  url = {https://doi.org/10.1038/s41567-019-0545-1},
  author = {Melko, Roger G. and Carleo, Giuseppe and Carrasquilla, Juan and Cirac, J. Ignacio},
  title = {Restricted Boltzmann machines in quantum physics},
  publisher = {Nature Physics},
  year = {2019},
}

@article{swinTransformerPaper,
  doi = {10.48550/ARXIV.2103.14030},
  url = {https://arxiv.org/abs/2103.14030},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{quantumMBPwithneuralNetworks,
	doi = {10.1126/science.aag2302},
	url = {https://doi.org/10.1126%2Fscience.aag2302},
	year = 2017,
	month = {feb},
	publisher = {American Association for the Advancement of Science ({AAAS})},
	volume = {355},
	number = {6325},
	pages = {602--606},
	author = {Giuseppe Carleo and Matthias Troyer},
	title = {Solving the quantum many-body problem with artificial neural networks},
	journal = {Science}
}

@online{jVMCgithub,
  author  = {Schmitt, Markus},
  title   = {jVMC},
  version = {1.1.2},
  url     = {https://github.com/markusschmitt/vmc_jax},
}

@online{cifarDataset,
  author  = {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
  title   = {The CIFAR-10 dataset},
  urldate = {2022-09-06},
  url     = {https://www.cs.toronto.edu/~kriz/cifar.html},
}

@article{imagenetDataset,
  Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  Title = {{ImageNet Large Scale Visual Recognition Challenge}},
  Year = {2015},
  journal   = {International Journal of Computer Vision (IJCV)},
  doi = {10.1007/s11263-015-0816-y},
  volume={115},
  number={3},
  pages={211-252}
}

@incollection{pytorchGithub,
  title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages = {8024--8035},
  year = {2019},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@online{jaxGithub,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@online{flaxGithub, 
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.6.0},
  year = {2020},
}

@online{dinoGithub,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e  and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},
  year={2021},
  url = {https://github.com/facebookresearch/dino},
  urldate = {2022-09-06},
}

@online{einopsGithub,
  title={Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},
  author={Alex Rogozhnikov},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=oapKSVM2bcj}
}

@online{positionalEncodingGithub,
    title={1D, 2D, and 3D Sinusoidal Postional Encoding (Pytorch and Tensorflow)},
    author={Peter Tatkowski},
    year={2021},
    url = {https://github.com/tatp22/multidim-positional-encoding},
    urldate = {2022-09-06},
}

@online{poolformerGithub,
    title={PoolFormer: MetaFormer Is Actually What You Need for Vision (CVPR 2022 Oral)},
    author={Sea AI Lab},
    year={2021},
    url = {https://github.com/sail-sg/poolformer},
    urldate = {2022-09-06},
}

@online{symmetricConvolutionImplementation,
  title={CNN-Symmetry},
  author={Matthias Treder},
  year={2019},
  url = {https://github.com/treder/CNN-Symmetry},
  urldate = {2022-09-06},
}

@article{backpropagationInConvolutions,
	url = {https://pavisj.medium.com/convolutions-and-backpropagations-46026a8f5d2c},
  urldate = {2022-09-06},
	year = 2018,
	publisher = {Medium},
	author = {Pavithra Solai},
	title = {Convolutions and Backpropagations},
}

@article{separableConvolutions,
	url = {https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec},
  urldate = {2022-09-06},
	year = 2018,
	publisher = {Medium},
	author = {Atul Pandey},
	title = {Depth-wise Convolution and Depth-wise Separable Convolution},
}

@online{hexagonalGrids,
  title={Hexagonal Grids},
  author={Amit Patel},
  year={2013},
  url = {https://www.redblobgames.com/grids/hexagons/},
  urldate = {2022-09-06},
}

@online{selfDocuments,
  title={Investigation of transformer architectures for geometrical graph structures and their application to two dimensional spin systems},
  author={Jonas Kell},
  year={2022},
  url = {https://github.com/jonas-kell/bachelor-thesis-documents},
}

@online{selfPhysics,
  title={Physis-Part implementation and Code Reference},
  author={Jonas Kell},
  year={2022},
  url = {https://github.com/jonas-kell/bachelor-thesis-code},
}

@online{selfComputerScience,
  title={Computer-Science-Part implementation and Code Reference},
  author={Jonas Kell},
  year={2022},
  url = {https://github.com/jonas-kell/bachelor-thesis-experiments},
}

@book{isingBook,
	doi = {10.1007/978-3-642-33039-1},
	url = {https://doi.org/10.1007/978-3-642-33039-1},
	year = 2013,
	publisher = {Springer Berlin, Heidelberg},
	author = {Sei Suzuki and Jun-ichi Inoue and Bikas K. Chakrabarti},
	title = {Quantum Ising Phases and Transitions in Transverse Ising Models},
}

@online{costDataStorage,
	title = {Historical cost of computer memory and storage},
	url = {https://ourworldindata.org/grapher/historical-cost-of-computer-memory-and-storage},
	abstract = {Measured in {US} dollars per megabyte.},
	titleaddon = {Our World in Data},
	urldate = {2022-09-09},
}

@online{dataCreatedRate,
	title = {How Much Data Is Created Every Day in 2022?},
	url = {https://techjury.net/blog/how-much-data-is-created-every-day/},
	abstract = {How much data is created every day? If "a lot" isn't enough for you, then check out our list of data growth statistics to find the answer!},
	titleaddon = {Techjury},
	author = {{Jacquelyn Bulao}},
	urldate = {2022-09-09},
	date = {2022-05-09},
	langid = {english},
}

@online{marketsizeArtificialIntelligence,
	title = {Artificial Intelligence Market Size Report, 2022-2030},
	url = {https://www.grandviewresearch.com/industry-analysis/artificial-intelligence-ai-market},
	abstract = {The global artificial intelligence market size was valued at {USD} 93.5 billion in 2021 and is projected to expand at a compound annual growth rate ({CAGR}) of 38.1\% from 2022 to 2030},
	urldate = {2022-09-09},
	langid = {english},
}

@article{quantumMonteCarloSimulationsOfSolids,
	title = {Quantum Monte Carlo simulations of solids},
	volume = {73},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.73.33},
	doi = {10.1103/RevModPhys.73.33},
	abstract = {This article describes the variational and fixed-node diffusion quantum Monte Carlo methods and how they may be used to calculate the properties of many-electron systems. These stochastic wave-function-based approaches provide a very direct treatment of quantum many-body effects and serve as benchmarks against which other techniques may be compared. They complement the less demanding density-functional approach by providing more accurate results and a deeper understanding of the physics of electronic correlation in real materials. The algorithms are intrinsically parallel, and currently available high-performance computers allow applications to systems containing a thousand or more electrons. With these tools one can study complicated problems such as the properties of surfaces and defects, while including electron correlation effects with high precision. The authors provide a pedagogical overview of the techniques and describe a selection of applications to ground and excited states of solids and clusters.},
	pages = {33--83},
	number = {1},
	journaltitle = {Reviews of Modern Physics},
	shortjournal = {Rev. Mod. Phys.},
	author = {Foulkes, W. M. C. and Mitas, L. and Needs, R. J. and Rajagopal, G.},
	urldate = {2022-09-09},
	date = {2001-01-05},
}

@misc{gpuPerformanceOverTime,
	title = {Summarizing {CPU} and {GPU} Design Trends with Product Data},
	url = {http://arxiv.org/abs/1911.11313},
	abstract = {Moore's Law and Dennard Scaling have guided the semiconductor industry for the past few decades. Recently, both laws have faced validity challenges as transistor sizes approach the practical limits of physics. We are interested in testing the validity of these laws and reflect on the reasons responsible. In this work, we collect data of more than 4000 publicly-available {CPU} and {GPU} products. We find that transistor scaling remains critical in keeping the laws valid. However, architectural solutions have become increasingly important and will play a larger role in the future. We observe that {GPUs} consistently deliver higher performance than {CPUs}. {GPU} performance continues to rise because of increases in {GPU} frequency, improvements in the thermal design power ({TDP}), and growth in die size. But we also see the ratio of {GPU} to {CPU} performance moving closer to parity, thanks to new {SIMD} extensions on {CPUs} and increased {CPU} core counts.},
	publisher = {{arXiv}},
	author = {Sun, Yifan and Agostini, Nicolas Bohm and Dong, Shi and Kaeli, David},
	urldate = {2022-09-09},
	date = {2020-07-13},
	eprinttype = {arxiv},
	eprint = {1911.11313 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@book{demtroderExperimentalphysik,
	location = {Berlin, Heidelberg},
	title = {Experimentalphysik 1},
	isbn = {9783662464144 9783662464151},
	url = {http://link.springer.com/10.1007/978-3-662-46415-1},
	series = {Springer-Lehrbuch},
	publisher = {Springer},
	author = {Demtröder, Wolfgang},
	urldate = {2022-09-09},
	date = {2015},
	doi = {10.1007/978-3-662-46415-1},
	keywords = {Einführung Experimentalphysik, Experimentalphysik Erklärt, Lehrbuch Demtröder, Lehrbuch Experimentalphysik, Lehrbuch Mechanik, Lehrbuch Schwingungen Wellen, Lehrbuch Thermodynamik, Lehrbuch Wärme, fluid- and aerodynamics},
}

@online{isingFerromagnetismn,
	title = {Beitrag zur Theorie des Ferromagnetismus {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/BF02980577},
	author = {{Ernst Ising}},
	urldate = {2022-09-09},
	date = {1925-02},
}


@book{schwablQM,
	location = {Berlin, Heidelberg},
	title = {Quantenmechanik ({QM} I)},
	isbn = {9783540736745},
	url = {http://link.springer.com/10.1007/978-3-540-73675-2},
	series = {Springer-Lehrbuch},
	publisher = {Springer},
	author = {{Franz Schwabl}},
	urldate = {2022-09-10},
	date = {2007},
	langid = {german},
	doi = {10.1007/978-3-540-73675-2},
	keywords = {Atomphysik, Kernphysik, Lehrbuch Quantenmechanik, Messprozesse Quantenmechanik, Molekülphysik, Quantenmechanik, Quantenphysik, Quantisierung Strahlungsfeld, Relativistische Korrekturen, Strahlungsfeld, Streutheorie, Supersymmetrie, Supersymmetrische Quant},
}

@online{frustration,
	title = {Neuartige Ordnungsphänomene in frustrierten Quantenmagneten},
	url = {https://www.mpg.de/357126/forschungsSchwerpunkt?c=147242},
	abstract = {Magnetische Spin-Systeme sind vor allem für ihre kollektive magnetische Ordnung bekannt. In diesem Übersichtsbeitrag werden neuartige Möglichkeiten aufgezeigt, wie sich wechselwirkende magnetische Systeme verhalten, wenn Quantenfluktuationen und magnetische Frustration kooperieren. Diese neuartigen Phasen erweitern unser Verständnis von quantenmechanischen Vielteilchensystemen und sind möglicherweise von experimenteller Relevanz für existierende magnetische Materialien bei niedrigen Temperaturen.},
	author = {Läuchli, Andreas},
	urldate = {2022-09-10},
	langid = {german},
}

@online{scipyeigsh,
	title = {scipy.sparse.linalg.eigsh — {SciPy} v1.9.1 Manual},
	url = {https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigsh.html#scipy.sparse.linalg.eigsh},
	urldate = {2022-09-11},
}

@online{avogadrosNumber,
	title = {{CODATA} Value: Avogadro constant},
	url = {https://physics.nist.gov/cgi-bin/cuu/Value?na},
	urldate = {2022-09-11},
}

@article{imagenarySchroedingerEquation,
title = {Integration of the Schrödinger equation in imaginary time},
journal = {Journal of Computational Physics},
volume = {1},
number = {3},
pages = {433-447},
year = {1967},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(67)90049-6},
url = {https://www.sciencedirect.com/science/article/pii/0021999167900496},
author = {Abraham Goldberg and Judah L Schwartz},
abstract = {Numerical techniques for the integration of the Schrödinger equation in imaginary time are investigated. Because the spatial dependence of the solution in the limit of large imaginary time is that of the ground state of the Hamiltonian, the method can be applied to bound states of quantum-mechanical three-body systems. Knowledge of the analytic dependence of the asymptotic form of the wavefunction on the trial eigenvalue is not required. In this paper, the first of a series, we have confined our attention to the problems arising from the imaginary time integration and reserve for a later date a discussion of the problems that are due to a multidimensional spatial grid.}
}

@article{ffnUnversalApproximator,
  title = {Multilayer feedforward networks are universal approximators},
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359-366},
  year = {1989},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}

@online{catPhoto,
  title = {Cat Photo},
  author = {Amber Kipp},
  url = {https://unsplash.com/photos/75715CVEJhI?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink},
	urldate = {2022-09-13},
}

@book{machineLearningMitchell,
	title = {Machine Learning},
	isbn = {9780070428072},
	series = {McGraw-Hill International Editions},
	publisher = {McGraw-Hill Education},
	author = {Mitchell, Tom M.},
	year={1997},
}