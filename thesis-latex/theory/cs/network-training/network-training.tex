% TODO 
% - Theory backpropagation
% - Optimizer
% - Hyperparameters
% - Augmentations
% - Maybe training hardware
% - batch size
% - loss function 

% SOURCE: \cite{mobileNetPaper}
% - explains distillation and other squeeze methods as complementary 

% SOURCE: \cite{dinoPaper}
% - discusses augmentations in depth

% SOURCE: \cite{convNetForThe2020s}
% - really big focus on the training procedures and hyperparameter design and augmentations 

