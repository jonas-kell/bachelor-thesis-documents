As a direct numerical approach is not sufficient, more advanced methods needed to be developed. 
\emph{Iterative} approaches have shown to be quite effective. 
In these kind of methods, the goal is not to compute a full solution directly, but to start with a random solution and perform iterative updates to it. 
If a good update strategy is chosen, the solution may converge towards the desired solution.

Because of the iterative nature, it is not necessary to do a complete computation at once. 
As learned in \fullref{sec:theory-numericalsolution}, the memory requirement for storing a complete hamiltonian matrix representation is also exponential.
So even with unlimited time, a numerical calculation could be impossible because of limited available memory. 
The iterative methods get around this, by being able to chose a data structure with much higher information density than the full hamiltonian (For example if only the ground state is wanted, it is unnecessary to store anything but information regarding it. The hamiltonian though encodes a lot of \glqq unnecessary\grqq{} information about other states). This structure can then be iteratively refined.

In practice a so called \emph{neural quantum state} (NQS) is used. 
This basically is nothing but a neural network, taken from established machine learning tasks.
The function is parametrized by $M$ variables $\theta_i \in \mathbb{R}, 0\leq i < M$ and takes a spin configuration as an input.
A spin configuration for $N$ lattice sites can be described by $\sigma_j \in \left\{\up, \down\right\}, 0\leq j < N$.
The function will be written like \autoref{eq:parametrized-wavefunction}.

\begin{equation}
    \label{eq:parametrized-wavefunction}
    \ket{\Psi_{\theta_0, ..., \theta_{M-1}}(\sigma_0, ..., \sigma_{N-1})} \equiv \ket{\Psi_\theta}
\end{equation}

In order to compute a helpful representation, the identities \autoref{eq:completeness-of-base} and \autoref{eq:wave-function-probability} are necessary \cite{schwablQM}. \autoref{eq:completeness-of-base} applies to \emph{complete orthonormal} bases only.

\begin{equation}
    \label{eq:completeness-of-base}
    \sum\limits_{s} \ket{s}\bra{s} = \mathbb{1}
\end{equation}

\begin{equation}
    \label{eq:wave-function-probability}
    \bra{\Psi}\ket{\Psi} = \Psi^\ast\cdot \Psi = \left|\Psi\right|^2 \in \mathbb{R}
\end{equation}

The following method is described in \cite{jVMCPaper}.

To calculate the \emph{expectation value} of a quantum mechanical operator \operator for the wavefunction $\ket{\Psi}$, the expression 
$\frac{\bra{\Psi}\operator\ket{\Psi}}{\bra{\Psi}\ket{\Psi}}$ is used \cite{schwablQM}. The denominator assures, that the calculation is correct for \emph{non-normalized} wavefunctions (as the NQS is initialized randomly it very likely isn't normalized). 

Applying this to the problem at hand, the target is to measure the observable corresponding to \operator in the parametrized neural quantum state $\ket{\Psi_\theta}$.

\begin{equation}
    \label{eq:derivation-local-estimator}
    \begin{split}
        \frac{\bra{\Psi_\theta}\operator\ket{\Psi_\theta}}{\bra{\Psi_\theta}\ket{\Psi_\theta}} &\stackrel{\ref{eq:completeness-of-base}}{=}
        %
        \frac{\bra{\Psi_\theta} \left(\sum\limits_{s} \ket{s}\bra{s}\right)
        \operator \left(\sum\limits_{s'} \ket{s'}\bra{s'}\right) \ket{\Psi_\theta}}{\bra{\Psi_\theta}\left(\sum\limits_{s} \ket{s}\bra{s}\right)\ket{\Psi_\theta}}\\
        %
        &\stackrel{\phantom{\ref{eq:completeness-of-base}}}{=}
        \frac{\sum\limits_{s, s'} \bra{\Psi_\theta} \ket{s}\bra{s}
        \operator \ket{s'}\bra{s'} \ket{\Psi_\theta}}{\sum\limits_{s}\bra{\Psi_\theta}\ket{s}\bra{s}\ket{\Psi_\theta}}
        %
        \stackrel{\phantom{\ref{eq:completeness-of-base}}}{=}
        \frac{\sum\limits_{s} \bra{\Psi_\theta} \ket{s} \frac{\bra{s} \ket{\Psi_\theta}}{\bra{s} \ket{\Psi_\theta}} \sum\limits_{s'} \bra{s}
        \operator \ket{s'}\bra{s'} \ket{\Psi_\theta}}{\sum\limits_{s}\bra{\Psi_\theta}\ket{s}\bra{s}\ket{\Psi_\theta}}\\
        %
        &\stackrel{\phantom{\ref{eq:completeness-of-base}}}{=}
        \sum\limits_{s} \frac{ \bra{\Psi_\theta} \ket{s}\bra{s} \ket{\Psi_\theta} \sum\limits_{s'} \bra{s}
        \operator \ket{s'} \frac{\bra{s'} \ket{\Psi_\theta}}{\bra{s} \ket{\Psi_\theta}}}{\bra{\Psi_\theta}\ket{s}\bra{s}\ket{\Psi_\theta}}
        %
        \stackrel{\ref{eq:local-estimator}}{=}
        \sum\limits_{s} \frac{ \bra{\Psi_\theta} \ket{s}\bra{s} \ket{\Psi_\theta} \operator_\mathrm{loc}^\theta(s)}{\bra{\Psi_\theta}\ket{s}\bra{s}\ket{\Psi_\theta}}\\
        %
        &\stackrel{\ref{eq:base-factors}}{=}
        \sum\limits_{s} \frac{ \Psi_\theta(s)^\ast \cdot \Psi_\theta(s) \cdot \operator_\mathrm{loc}^\theta(s)}{\Psi_\theta(s)^\ast \cdot \Psi_\theta(s)}
        %
        \stackrel{\ref{eq:wave-function-probability}}{=}
        \sum\limits_{s} \frac{ \left|\Psi_\theta(s)\right|^2  \operator_\mathrm{loc}^\theta(s)}{\left|\Psi_\theta(s)\right|^2}
    \end{split}
\end{equation}

In the derivation, the \emph{local estimator} (\autoref{eq:local-estimator}) is introduced. Because the computational base is sparse, only very few elements of the sum over $s'$ in \autoref{eq:local-estimator} are non-zero \cite{jVMCPaper}. A local estimator can therefore be evaluated in constant time.

\begin{equation}
    \label{eq:local-estimator}
    \operator_\mathrm{loc}^\theta(s) = \sum\limits_{s'} \bra{s} \operator \ket{s'} \frac{\bra{s'} \ket{\Psi_\theta}}{\bra{s} \ket{\Psi_\theta}}
\end{equation}

The final result in \autoref{eq:derivation-local-estimator} has the form of a \emph{probability distribution} over $s$.
So the calculation of a expectation value can be re-formulated as the calculation of the expectation value of $s$ of the local estimator (which can be evaluated in constant time).

This alone is not helpful, because $\ket{s}$ is still exponentially large in regards to the problem size.
But this shortcoming can be resolved with the introduction of \emph{Monte Carlo} sampling.
The \emph{probability density} can be estimated, by looking at only a subset of possible $\ket{s}$.

By randomly sampling a large, but manageable number $N_\mathrm{MC}$ of states $s_{(n)}$ with the \emph{Metropolis algorithm} \cite{quantumMonteCarloSimulationsOfSolids}, the true value can be approximated (\autoref{eq:mc-local-estimator}) \cite{jVMCPaper}.

\begin{equation}
    \label{eq:mc-local-estimator}
    \frac{\bra{\Psi_\theta}\operator\ket{\Psi_\theta}}{\bra{\Psi_\theta}\ket{\Psi_\theta}} \approx
    \frac{1}{N_\mathrm{MC}} \sum\limits_{n=1}^{N_\mathrm{MC}} \operator_\mathrm{loc}^\theta(s_{(n)})
\end{equation}

Therefore the energy of a system can be estimated with \autoref{eq:energy-optimization}. All the terms can efficiently be computed, using methods from \fullref{sec:theory-numericalsolution}.

\begin{equation}
    \label{eq:energy-optimization}
    E(\theta) = \frac{\bra{\Psi_\theta}\hamiltonian\ket{\Psi_\theta}}{\bra{\Psi_\theta}\ket{\Psi_\theta}} \approx
    \frac{1}{N_\mathrm{MC}} \sum\limits_{n=1}^{N_\mathrm{MC}} \hamiltonian_\mathrm{loc}^\theta(s_{(n)}) \stackrel{\ref{eq:local-estimator}}{=}
    \frac{1}{N_\mathrm{MC}} \sum\limits_{n=1}^{N_\mathrm{MC}} \left(\sum\limits_{s'} \bra{s_{(n)}} \hamiltonian \ket{s'} \frac{\bra{s'} \ket{\Psi_\theta}}{\bra{s_{(n)}} \ket{\Psi_\theta}}\right)
\end{equation}

As it is known, that the ground state energy $E_0$ is the smallest energy, the ground state can be estimated in the following way:
\begin{enumerate}
    \setlength\itemsep{-0.5em}
    \item Start with a randomly initialized NQS
    \item Estimate its energy with \autoref{eq:energy-optimization} and MC-sampled states
    \item Calculate the neural network error, assuming $E(\theta)$ should be smaller (formula in \cite{jVMCPaper})
    \item Propagate the error back into the network
    \item Repeat at 2. with the updated NQS
\end{enumerate}

Because the method iteratively updates the network by performing tiny variations, it is called \emph{variational Monte Carlo} (VMC) method.
